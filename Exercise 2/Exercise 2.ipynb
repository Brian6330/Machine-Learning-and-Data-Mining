{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Naive Bayes\n",
    "\n",
    "First name: Brian\n",
    "<br>\n",
    "Last name: Schweigler\n",
    "<br>\n",
    "Matriculation number: 16-102-071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1a) In the correlation visualization, select the two features that have the most significant correlation to the target feature, Survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "                                                    Survived  Pclass     Sex  \\\n",
      "Name                                                                           \n",
      "Mr. Owen Harris Braund                                     0       3    male   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings         1       1  female   \n",
      "Miss. Laina Heikkinen                                      1       3  female   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                1       1  female   \n",
      "Mr. William Henry Allen                                    0       3    male   \n",
      "\n",
      "                                                     Age  \\\n",
      "Name                                                       \n",
      "Mr. Owen Harris Braund                              22.0   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings  38.0   \n",
      "Miss. Laina Heikkinen                               26.0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle         35.0   \n",
      "Mr. William Henry Allen                             35.0   \n",
      "\n",
      "                                                    Siblings/Spouses Aboard  \\\n",
      "Name                                                                          \n",
      "Mr. Owen Harris Braund                                                    1   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings                        1   \n",
      "Miss. Laina Heikkinen                                                     0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                               1   \n",
      "Mr. William Henry Allen                                                   0   \n",
      "\n",
      "                                                    Parents/Children Aboard  \\\n",
      "Name                                                                          \n",
      "Mr. Owen Harris Braund                                                    0   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings                        0   \n",
      "Miss. Laina Heikkinen                                                     0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                               0   \n",
      "Mr. William Henry Allen                                                   0   \n",
      "\n",
      "                                                       Fare  \n",
      "Name                                                         \n",
      "Mr. Owen Harris Braund                               7.2500  \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings  71.2833  \n",
      "Miss. Laina Heikkinen                                7.9250  \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle         53.1000  \n",
      "Mr. William Henry Allen                              8.0500  \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "from mlxtend.classifier import OneRClassifier\n",
    "from mlxtend.evaluate import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"data/titanic.csv\", index_col='Name')\n",
    "# df.describe(include='all')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.head(5))\n",
    "le = preprocessing.LabelEncoder()\n",
    "#df[\"Name\"] = le.fit_transform(df[\"Name\"])\n",
    "df[\"Survived\"] = le.fit_transform(df[\"Survived\"])\n",
    "df[\"Sex\"] = le.fit_transform(df[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of columns to column survived\n",
      "Survived                   1.000000\n",
      "Sex                        0.542152\n",
      "Pclass                     0.336528\n",
      "Fare                       0.256179\n",
      "Parents/Children Aboard    0.080097\n",
      "Age                        0.059665\n",
      "Siblings/Spouses Aboard    0.037082\n",
      "Name: Survived, dtype: float64\n",
      "Sex has the highest correlation, Pclass the second highest\n",
      "Accuracy if assuming everyone survived : 0.3855693348365276\n",
      "Accuracy if assuming everyone died : 0.6144306651634723\n"
     ]
    }
   ],
   "source": [
    "correlation = df.corr()[\"Survived\"]\n",
    "correlation = correlation.apply(lambda entry: abs(entry))\n",
    "\n",
    "print(\"Correlation of columns to column survived\")\n",
    "print(correlation.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "correlation.pop(correlation.idxmax())\n",
    "temp_correlation = correlation\n",
    "best = correlation.idxmax()\n",
    "temp_correlation.pop(best)\n",
    "second_best = temp_correlation.idxmax()\n",
    "\n",
    "print(best + \" has the highest correlation, \" + second_best + \" the second highest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (1b) Using Naive Bayes classifier and the most two significant features to predict the Survival of the travellers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\JetBrains\\PyCharm 2021.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_utils.py\", line 64, in attach_to_debugger\n",
      "    debugger.prepare_to_run(enable_tracing_from_start=False)\n",
      "TypeError: PyDB.prepare_to_run() got an unexpected keyword argument 'enable_tracing_from_start'\n",
      "Failed to connect to target debugger.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('Sex', 'Pclass')",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3799\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3800\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3801\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: ('Sex', 'Pclass')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11040/559260913.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# pydev_debug_cell\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mnaive_bayes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGaussianNB\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mbest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msecond_best\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Survived\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m6\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mnaive_bayes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mtest_predictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnaive_bayes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3803\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3804\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3805\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3806\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3807\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3800\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3801\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3802\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3803\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3804\u001B[0m                 \u001B[1;31m# If we have a listlike key, _check_indexing_error will raise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: ('Sex', 'Pclass')"
     ]
    }
   ],
   "source": [
    "naive_bayes = GaussianNB()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[best, second_best], df[\"Survived\"], test_size=0.4, random_state=6)\n",
    "naive_bayes.fit(x_train, y_train)\n",
    "test_predictions = naive_bayes.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f'Accuracy: {round(accuracy,4)} for Naive Bayes based on the two best correlation values.' )\n",
    "\n",
    "#Select the two highest correlated predictors. and the targe value\n",
    "\n",
    "#Now lets train and evaluate the model based on Sex and class:\n",
    "\n",
    "\n",
    "\n",
    "# survived = 0\n",
    "# died = 0\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[\"Survived\"] == 1:\n",
    "#         survived += 1\n",
    "#     else:\n",
    "#         died += 1\n",
    "#\n",
    "# print(\"Accuracy if assuming everyone survived :\", survived/(survived+died))\n",
    "# print(\"Accuracy if assuming everyone died :\", died/(survived+died))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, \"choose most frequent to be the default rule\".\n",
    "In this case, assuming that everyone died is the best default rule, and we should aim to beat with our prediction approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1b) What is the best 1R for this dataset?\n",
    "Very likely, without prior information, we can assume that due to \"mothers and children first\" when the titanic sank,\n",
    "that those are most likely to have survived, thus gender (Sex) or if they are a parent/child.\n",
    "\n",
    "So let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_d = df[[\"Sex\"]]\n",
    "y = df[\"Survived\"]\n",
    "Xd_train, Xd_test, y_train, y_test = train_test_split(X_d, y, test_size=0.3, random_state=2)\n",
    "oner = OneRClassifier()\n",
    "oner.fit(Xd_train.to_numpy(), y_train)\n",
    "y_pred = oner.predict(Xd_test.to_numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy using outdated binary gender: \", accuracy)\n",
    "\n",
    "X_d_fam = df[[\"Parents/Children Aboard\"]]\n",
    "y_fam = df[\"Survived\"]\n",
    "Xd_train_fam, Xd_test_fam, y_train_fam, y_test_fam = train_test_split(X_d_fam, y_fam, test_size=0.3, random_state=2)\n",
    "oner_fam = OneRClassifier()\n",
    "oner_fam.fit(Xd_train_fam.to_numpy(), y_train_fam)\n",
    "y_pred_fam = oner_fam.predict(Xd_test_fam.to_numpy())\n",
    "\n",
    "accuracy_fam = accuracy_score(y_test_fam, y_pred_fam)\n",
    "print(\"Parents/Children accuracy: \", accuracy_fam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the outdated binary gender within this dataset (M/F) is the best 1R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1c) Can you produce a second rule based on a single attribute with a good effectiveness?\n",
    "For this we can simply look at all the variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_d_fare = df[[\"Fare\"]]\n",
    "y_fare = df[\"Survived\"]\n",
    "Xd_train_fare, Xd_test_fare, y_train_fare, y_test_fare = train_test_split(X_d_fare, y_fare, test_size=0.3, random_state=2)\n",
    "oner_fare = OneRClassifier()\n",
    "oner_fare.fit(Xd_train_fare.to_numpy(), y_train_fare)\n",
    "y_pred_fare = oner_fare.predict(Xd_test_fare.to_numpy())\n",
    "accuracy_fare = accuracy_score(y_test_fare, y_pred_fare)\n",
    "print(\"Fare Accuracy: \", accuracy_fare)\n",
    "\n",
    "X_d_pclass = df[[\"Pclass\"]]\n",
    "y_pclass = df[\"Survived\"]\n",
    "Xd_train_pclass, Xd_test_pclass, y_train_pclass, y_test_pclass = train_test_split(X_d_pclass, y_pclass, test_size=0.3, random_state=2)\n",
    "oner_pclass = OneRClassifier()\n",
    "oner_pclass.fit(Xd_train_pclass.to_numpy(), y_train_pclass)\n",
    "y_pred_pclass = oner_pclass.predict(Xd_test_pclass.to_numpy())\n",
    "\n",
    "accuracy_pclass = accuracy_score(y_test_pclass, y_pred_pclass)\n",
    "print(\"Pclass Accuracy: \", accuracy_pclass)\n",
    "\n",
    "X_d_age = df[[\"Age\"]]\n",
    "y_age = df[\"Survived\"]\n",
    "Xd_train_age, Xd_test_age, y_train_age, y_test_age = train_test_split(X_d_age, y_age, test_size=0.3, random_state=2)\n",
    "oner_age = OneRClassifier()\n",
    "oner_age.fit(Xd_train_age.to_numpy(), y_train_age)\n",
    "y_pred_age = oner_age.predict(Xd_test_age.to_numpy())\n",
    "\n",
    "accuracy_age = accuracy_score(y_test_age, y_pred_age)\n",
    "print(\"Age accuracy: \", accuracy_age)\n",
    "\n",
    "X_d_sib = df[[\"Siblings/Spouses Aboard\"]]\n",
    "y_sib = df[\"Survived\"]\n",
    "Xd_train_sib, Xd_test_sib, y_train_sib, y_test_sib = train_test_split(X_d_sib, y_sib, test_size=0.3, random_state=2)\n",
    "oner_sib = OneRClassifier()\n",
    "oner_sib.fit(Xd_train_sib.to_numpy(), y_train_sib)\n",
    "y_pred_sib = oner_sib.predict(Xd_test_sib.to_numpy())\n",
    "\n",
    "accuracy_sib = accuracy_score(y_test_sib, y_pred_sib)\n",
    "print(\"Sibling/Spouse accuracy: \", accuracy_sib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thus, we can conclude that using the Pclass (or Fare which correlates with it) are the best ones.\n",
    "This is likely due to the fact that those who payed more (or are in a better class) are higher up on the ship, farther away from the engines.\n",
    "Thus they had more time to escape.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Use a stock / market index for daily return of the day\n",
    "First we simply load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "stock_df = pd.read_csv(\"data/Nasdaq.csv\", index_col='Date')\n",
    "print(stock_df.head(10))\n",
    "# stock_df['Date'] = stock_df['Date'].apply(pd.to_datetime)\n",
    "stock_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at daily return histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "daily_return = np.empty(stock_df['Close'].shape)\n",
    "#  From Slides: Daily return (r): Difference in percentage between the price at time t+1 and at time t\n",
    "daily_return[0] = float('NaN') # The first\n",
    "daily_return[1:] = np.ediff1d(stock_df['Close']) / stock_df['Close'][:-1]\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Daily Return', value=daily_return)\n",
    "\n",
    "bins = int(len(daily_return) / 32)\n",
    "plt.hist(daily_return, bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to compare binary and ternary solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary = (daily_return > 0).astype(float)\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Binary Decision', value=binary)\n",
    "\n",
    "limit = 0.001\n",
    "ternary = np.zeros(daily_return.shape)\n",
    "ternary[np.where(daily_return > limit)] = 2\n",
    "ternary[np.where(daily_return < limit)] = 1\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Ternary Decision', value=ternary)\n",
    "\n",
    "print(stock_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1R model using volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_d_stock = stock_df[[\"Volume\"]]\n",
    "y_stock = stock_df[\"Binary Decision\"]\n",
    "print(\"Length of DF: \", stock_df.shape[0])\n",
    "test_size = 100/stock_df.shape[0]\n",
    "print(\"Percentage of test_size to use last 100 days: \", test_size)\n",
    "Xd_train_stock, Xd_test_stock, y_train_stock, y_test_stock = train_test_split(X_d_stock, y_stock, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock = OneRClassifier()\n",
    "oner_stock.fit(Xd_train_stock.to_numpy(), y_train_stock)\n",
    "y_pred_stock = oner_stock.predict(Xd_test_stock.to_numpy())\n",
    "accuracy_stock_bi = accuracy_score(y_test_stock, y_pred_stock)\n",
    "print(\"Binary Accuracy: \", accuracy_stock_bi)\n",
    "\n",
    "# As ternary does not play nicely with OneR, using binary only\n",
    "# X_d_stock_ter = stock_df[[\"Volume\"]]\n",
    "# y_stock_ter = stock_df[\"Ternary Decision\"]\n",
    "# Xd_train_stock_ter, Xd_test_stock_ter, y_train_stock_ter, y_test_stock_ter = train_test_split(X_d_stock_ter, y_stock_ter, test_size=0.3)\n",
    "# oner_stock_ter = OneRClassifier()\n",
    "# oner_stock_ter.fit(Xd_train_stock_ter.to_numpy(), y_train_stock_ter)\n",
    "# y_pred_stock_ter = oner_stock_ter.predict(Xd_test_stock_ter.to_numpy())\n",
    "# accuracy_stock_ter = accuracy_score(y_test_stock_ter, y_pred_stock_ter)\n",
    "# print(\"Ternary Accuracy: \", accuracy_stock_ter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1R model using volume and rolling averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stock_df['Rolling Mean 5'] = stock_df['Close'].rolling(5).mean()\n",
    "stock_df['Rolling Mean 10'] = stock_df['Close'].rolling(10).mean()\n",
    "stock_df['Rolling Mean 20'] = stock_df['Close'].rolling(20).mean()\n",
    "stock_df['Rolling Mean 50'] = stock_df['Close'].rolling(50).mean()\n",
    "stock_df['Rolling Mean 200'] = stock_df['Close'].rolling(200).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_d_stock_5 = stock_df[[\"Volume\", \"Rolling Mean 5\"]]\n",
    "y_stock_5 = stock_df[\"Binary Decision\"]\n",
    "test_size = 100/stock_df.shape[0]\n",
    "Xd_train_stock_5, Xd_test_stock_5, y_train_stock_5, y_test_stock_5 = train_test_split(X_d_stock_5, y_stock_5, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_5 = OneRClassifier()\n",
    "oner_stock_5.fit(Xd_train_stock_5.to_numpy(), y_train_stock_5)\n",
    "y_pred_stock_5 = oner_stock_5.predict(Xd_test_stock_5.to_numpy())\n",
    "accuracy_stock_bi_5 = accuracy_score(y_test_stock_5, y_pred_stock_5)\n",
    "print(\"Rolling Mean 5 Accuracy: \", accuracy_stock_bi_5)\n",
    "\n",
    "X_d_stock_10 = stock_df[[\"Volume\", \"Rolling Mean 10\"]]\n",
    "y_stock_10 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_10, Xd_test_stock_10, y_train_stock_10, y_test_stock_10 = train_test_split(X_d_stock_10, y_stock_10, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_10 = OneRClassifier()\n",
    "oner_stock_10.fit(Xd_train_stock_10.to_numpy(), y_train_stock_10)\n",
    "y_pred_stock_10 = oner_stock.predict(Xd_test_stock_10.to_numpy())\n",
    "accuracy_stock_bi_10 = accuracy_score(y_test_stock_10, y_pred_stock_10)\n",
    "print(\"Rolling Mean 10 Accuracy: \", accuracy_stock_bi_10)\n",
    "\n",
    "X_d_stock_20 = stock_df[[\"Volume\", \"Rolling Mean 20\"]]\n",
    "y_stock_20 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_20, Xd_test_stock_20, y_train_stock_20, y_test_stock_20 = train_test_split(X_d_stock_20, y_stock_20, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_20 = OneRClassifier()\n",
    "oner_stock_20.fit(Xd_train_stock_20.to_numpy(), y_train_stock_20)\n",
    "y_pred_stock_20 = oner_stock_20.predict(Xd_test_stock_20.to_numpy())\n",
    "accuracy_stock_bi_20 = accuracy_score(y_test_stock_20, y_pred_stock_20)\n",
    "print(\"Rolling Mean 20 Accuracy: \", accuracy_stock_bi_20)\n",
    "\n",
    "X_d_stock_50 = stock_df[[\"Volume\", \"Rolling Mean 50\"]]\n",
    "y_stock_50 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_50, Xd_test_stock_50, y_train_stock_50, y_test_stock_50 = train_test_split(X_d_stock_50, y_stock_50, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_50 = OneRClassifier()\n",
    "oner_stock_50.fit(Xd_train_stock_50.to_numpy(), y_train_stock_50)\n",
    "y_pred_stock_50 = oner_stock_50.predict(Xd_test_stock_50.to_numpy())\n",
    "accuracy_stock_bi_50 = accuracy_score(y_test_stock_50, y_pred_stock_50)\n",
    "print(\"Rolling Mean 50 Accuracy: \", accuracy_stock_bi_50)\n",
    "\n",
    "X_d_stock_200 = stock_df[[\"Volume\", \"Rolling Mean 200\"]]\n",
    "y_stock_200 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_200, Xd_test_stock_200, y_train_stock_200, y_test_stock_200 = train_test_split(X_d_stock_200, y_stock_200, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_200 = OneRClassifier()\n",
    "oner_stock_200.fit(Xd_train_stock_200.to_numpy(), y_train_stock_200)\n",
    "y_pred_stock_200 = oner_stock_200.predict(Xd_test_stock_200.to_numpy())\n",
    "accuracy_stock_bi_200 = accuracy_score(y_test_stock_200, y_pred_stock_200)\n",
    "print(\"Rolling Mean 200 Accuracy: \", accuracy_stock_bi_200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly enough, no matter what the rolling average, we seem to receive the same results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}