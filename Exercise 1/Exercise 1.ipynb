{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Simple rules\n",
    "\n",
    "First name: Brian\n",
    "<br>\n",
    "Last name: Schweigler\n",
    "<br>\n",
    "Matriculation number: 16-102-071"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1a) What is the best default rule for this dataset? (Default means without any evidence about the person)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without prior knowledge, all we know is whether a person died or not.\n",
    "For this, we can test the two rules \"all dead\" or all \"survived\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "                                                    Survived  Pclass     Sex  \\\n",
      "Name                                                                           \n",
      "Mr. Owen Harris Braund                                     0       3    male   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings         1       1  female   \n",
      "Miss. Laina Heikkinen                                      1       3  female   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                1       1  female   \n",
      "Mr. William Henry Allen                                    0       3    male   \n",
      "Mr. James Moran                                            0       3    male   \n",
      "Mr. Timothy J McCarthy                                     0       1    male   \n",
      "Master. Gosta Leonard Palsson                              0       3    male   \n",
      "Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson           1       3  female   \n",
      "Mrs. Nicholas (Adele Achem) Nasser                         1       2  female   \n",
      "\n",
      "                                                     Age  \\\n",
      "Name                                                       \n",
      "Mr. Owen Harris Braund                              22.0   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings  38.0   \n",
      "Miss. Laina Heikkinen                               26.0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle         35.0   \n",
      "Mr. William Henry Allen                             35.0   \n",
      "Mr. James Moran                                     27.0   \n",
      "Mr. Timothy J McCarthy                              54.0   \n",
      "Master. Gosta Leonard Palsson                        2.0   \n",
      "Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson    27.0   \n",
      "Mrs. Nicholas (Adele Achem) Nasser                  14.0   \n",
      "\n",
      "                                                    Siblings/Spouses Aboard  \\\n",
      "Name                                                                          \n",
      "Mr. Owen Harris Braund                                                    1   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings                        1   \n",
      "Miss. Laina Heikkinen                                                     0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                               1   \n",
      "Mr. William Henry Allen                                                   0   \n",
      "Mr. James Moran                                                           0   \n",
      "Mr. Timothy J McCarthy                                                    0   \n",
      "Master. Gosta Leonard Palsson                                             3   \n",
      "Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson                          0   \n",
      "Mrs. Nicholas (Adele Achem) Nasser                                        1   \n",
      "\n",
      "                                                    Parents/Children Aboard  \\\n",
      "Name                                                                          \n",
      "Mr. Owen Harris Braund                                                    0   \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings                        0   \n",
      "Miss. Laina Heikkinen                                                     0   \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle                               0   \n",
      "Mr. William Henry Allen                                                   0   \n",
      "Mr. James Moran                                                           0   \n",
      "Mr. Timothy J McCarthy                                                    0   \n",
      "Master. Gosta Leonard Palsson                                             1   \n",
      "Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson                          2   \n",
      "Mrs. Nicholas (Adele Achem) Nasser                                        0   \n",
      "\n",
      "                                                       Fare  \n",
      "Name                                                         \n",
      "Mr. Owen Harris Braund                               7.2500  \n",
      "Mrs. John Bradley (Florence Briggs Thayer) Cumings  71.2833  \n",
      "Miss. Laina Heikkinen                                7.9250  \n",
      "Mrs. Jacques Heath (Lily May Peel) Futrelle         53.1000  \n",
      "Mr. William Henry Allen                              8.0500  \n",
      "Mr. James Moran                                      8.4583  \n",
      "Mr. Timothy J McCarthy                              51.8625  \n",
      "Master. Gosta Leonard Palsson                       21.0750  \n",
      "Mrs. Oscar W (Elisabeth Vilhelmina Berg) Johnson    11.1333  \n",
      "Mrs. Nicholas (Adele Achem) Nasser                  30.0708  \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "import scipy\n",
    "from mlxtend.classifier import OneRClassifier\n",
    "from mlxtend.evaluate import accuracy_score\n",
    "\n",
    "clf = GaussianNB()\n",
    "le = preprocessing.LabelEncoder()\n",
    "df = pd.read_csv(\"data/titanic.csv\", index_col='Name')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.head(10))\n",
    "# df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy if assuming everyone survived : 0.3855693348365276\n",
      "Accuracy if assuming everyone died : 0.6144306651634723\n"
     ]
    }
   ],
   "source": [
    "survived = 0\n",
    "died = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"Survived\"] == 1:\n",
    "        survived += 1\n",
    "    else:\n",
    "        died += 1\n",
    "\n",
    "print(\"Accuracy if assuming everyone survived :\", survived/(survived+died))\n",
    "print(\"Accuracy if assuming everyone died :\", died/(survived+died))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, \"choose most frequent to be the default rule\".\n",
    "In this case, assuming that everyone died is the best default rule, and we should aim to beat with our prediction approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1b) What is the best 1R for this dataset?\n",
    "Very likely, without prior information, we can assume that due to \"mothers and children first\" when the titanic sank,\n",
    "that those are most likely to have survived, thus gender (Sex) or if they are a parent/child.\n",
    "\n",
    "So let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using outdated binary gender:  0.8014981273408239\n",
      "Parents/Children accuracy:  0.602996254681648\n"
     ]
    }
   ],
   "source": [
    "X_d = df[[\"Sex\"]]\n",
    "y = df[\"Survived\"]\n",
    "Xd_train, Xd_test, y_train, y_test = train_test_split(X_d, y, test_size=0.3, random_state=2)\n",
    "oner = OneRClassifier()\n",
    "oner.fit(Xd_train.to_numpy(), y_train)\n",
    "y_pred = oner.predict(Xd_test.to_numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy using outdated binary gender: \", accuracy)\n",
    "\n",
    "X_d_fam = df[[\"Parents/Children Aboard\"]]\n",
    "y_fam = df[\"Survived\"]\n",
    "Xd_train_fam, Xd_test_fam, y_train_fam, y_test_fam = train_test_split(X_d_fam, y_fam, test_size=0.3, random_state=2)\n",
    "oner_fam = OneRClassifier()\n",
    "oner_fam.fit(Xd_train_fam.to_numpy(), y_train_fam)\n",
    "y_pred_fam = oner_fam.predict(Xd_test_fam.to_numpy())\n",
    "\n",
    "accuracy_fam = accuracy_score(y_test_fam, y_pred_fam)\n",
    "print(\"Parents/Children accuracy: \", accuracy_fam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the outdated binary gender within this dataset (M/F) is the best 1R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1c) Can you produce a second rule based on a single attribute with a good effectiveness?\n",
    "For this we can simply look at all the variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fare Accuracy:  0.6853932584269663\n",
      "Pclass Accuracy:  0.6853932584269663\n",
      "Age accuracy:  0.5917602996254682\n",
      "Sibling/Spouse accuracy:  0.6367041198501873\n"
     ]
    }
   ],
   "source": [
    "X_d_fare = df[[\"Fare\"]]\n",
    "y_fare = df[\"Survived\"]\n",
    "Xd_train_fare, Xd_test_fare, y_train_fare, y_test_fare = train_test_split(X_d_fare, y_fare, test_size=0.3, random_state=2)\n",
    "oner_fare = OneRClassifier()\n",
    "oner_fare.fit(Xd_train_fare.to_numpy(), y_train_fare)\n",
    "y_pred_fare = oner_fare.predict(Xd_test_fare.to_numpy())\n",
    "accuracy_fare = accuracy_score(y_test_fare, y_pred_fare)\n",
    "print(\"Fare Accuracy: \", accuracy_fare)\n",
    "\n",
    "X_d_pclass = df[[\"Pclass\"]]\n",
    "y_pclass = df[\"Survived\"]\n",
    "Xd_train_pclass, Xd_test_pclass, y_train_pclass, y_test_pclass = train_test_split(X_d_pclass, y_pclass, test_size=0.3, random_state=2)\n",
    "oner_pclass = OneRClassifier()\n",
    "oner_pclass.fit(Xd_train_pclass.to_numpy(), y_train_pclass)\n",
    "y_pred_pclass = oner_pclass.predict(Xd_test_pclass.to_numpy())\n",
    "\n",
    "accuracy_pclass = accuracy_score(y_test_pclass, y_pred_pclass)\n",
    "print(\"Pclass Accuracy: \", accuracy_pclass)\n",
    "\n",
    "X_d_age = df[[\"Age\"]]\n",
    "y_age = df[\"Survived\"]\n",
    "Xd_train_age, Xd_test_age, y_train_age, y_test_age = train_test_split(X_d_age, y_age, test_size=0.3, random_state=2)\n",
    "oner_age = OneRClassifier()\n",
    "oner_age.fit(Xd_train_age.to_numpy(), y_train_age)\n",
    "y_pred_age = oner_age.predict(Xd_test_age.to_numpy())\n",
    "\n",
    "accuracy_age = accuracy_score(y_test_age, y_pred_age)\n",
    "print(\"Age accuracy: \", accuracy_age)\n",
    "\n",
    "X_d_sib = df[[\"Siblings/Spouses Aboard\"]]\n",
    "y_sib = df[\"Survived\"]\n",
    "Xd_train_sib, Xd_test_sib, y_train_sib, y_test_sib = train_test_split(X_d_sib, y_sib, test_size=0.3, random_state=2)\n",
    "oner_sib = OneRClassifier()\n",
    "oner_sib.fit(Xd_train_sib.to_numpy(), y_train_sib)\n",
    "y_pred_sib = oner_sib.predict(Xd_test_sib.to_numpy())\n",
    "\n",
    "accuracy_sib = accuracy_score(y_test_sib, y_pred_sib)\n",
    "print(\"Sibling/Spouse accuracy: \", accuracy_sib)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thus, we can conclude that using the Pclass (or Fare which correlates with it) are the best ones.\n",
    "This is likely due to the fact that those who payed more (or are in a better class) are higher up on the ship, farther away from the engines.\n",
    "Thus they had more time to escape.\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Use a stock / market index for daily return of the day\n",
    "First we simply load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  Volume\n",
      "Date                                                                          \n",
      "1971-02-05  100.000000  100.000000  100.000000  100.000000  100.000000       0\n",
      "1971-02-08  100.839996  100.839996  100.839996  100.839996  100.839996       0\n",
      "1971-02-09  100.760002  100.760002  100.760002  100.760002  100.760002       0\n",
      "1971-02-10  100.690002  100.690002  100.690002  100.690002  100.690002       0\n",
      "1971-02-11  101.449997  101.449997  101.449997  101.449997  101.449997       0\n",
      "1971-02-12  102.050003  102.050003  102.050003  102.050003  102.050003       0\n",
      "1971-02-16  102.190002  102.190002  102.190002  102.190002  102.190002       0\n",
      "1971-02-17  101.739998  101.739998  101.739998  101.739998  101.739998       0\n",
      "1971-02-18  101.419998  101.419998  101.419998  101.419998  101.419998       0\n",
      "1971-02-19  100.699997  100.699997  100.699997  100.699997  100.699997       0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12402.000000</td>\n",
       "      <td>12402.000000</td>\n",
       "      <td>12402.000000</td>\n",
       "      <td>12402.000000</td>\n",
       "      <td>12402.000000</td>\n",
       "      <td>1.240200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2120.657774</td>\n",
       "      <td>2134.377615</td>\n",
       "      <td>2104.791436</td>\n",
       "      <td>2120.585484</td>\n",
       "      <td>2120.585484</td>\n",
       "      <td>1.042295e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2692.888884</td>\n",
       "      <td>2709.482136</td>\n",
       "      <td>2673.491669</td>\n",
       "      <td>2693.171132</td>\n",
       "      <td>2693.171132</td>\n",
       "      <td>1.145532e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>54.869999</td>\n",
       "      <td>54.869999</td>\n",
       "      <td>54.869999</td>\n",
       "      <td>54.869999</td>\n",
       "      <td>54.869999</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>284.700012</td>\n",
       "      <td>284.857498</td>\n",
       "      <td>284.150002</td>\n",
       "      <td>284.477501</td>\n",
       "      <td>284.477501</td>\n",
       "      <td>5.035250e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1262.969971</td>\n",
       "      <td>1268.849976</td>\n",
       "      <td>1250.440002</td>\n",
       "      <td>1259.405029</td>\n",
       "      <td>1259.405029</td>\n",
       "      <td>5.930600e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2623.209961</td>\n",
       "      <td>2642.297607</td>\n",
       "      <td>2599.407410</td>\n",
       "      <td>2619.670044</td>\n",
       "      <td>2619.670044</td>\n",
       "      <td>1.870712e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15375.980469</td>\n",
       "      <td>15403.440430</td>\n",
       "      <td>15343.280273</td>\n",
       "      <td>15374.330078</td>\n",
       "      <td>15374.330078</td>\n",
       "      <td>1.110216e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open          High           Low         Close     Adj Close  \\\n",
       "count  12402.000000  12402.000000  12402.000000  12402.000000  12402.000000   \n",
       "mean    2120.657774   2134.377615   2104.791436   2120.585484   2120.585484   \n",
       "std     2692.888884   2709.482136   2673.491669   2693.171132   2693.171132   \n",
       "min       54.869999     54.869999     54.869999     54.869999     54.869999   \n",
       "25%      284.700012    284.857498    284.150002    284.477501    284.477501   \n",
       "50%     1262.969971   1268.849976   1250.440002   1259.405029   1259.405029   \n",
       "75%     2623.209961   2642.297607   2599.407410   2619.670044   2619.670044   \n",
       "max    15375.980469  15403.440430  15343.280273  15374.330078  15374.330078   \n",
       "\n",
       "             Volume  \n",
       "count  1.240200e+04  \n",
       "mean   1.042295e+09  \n",
       "std    1.145532e+09  \n",
       "min    0.000000e+00  \n",
       "25%    5.035250e+07  \n",
       "50%    5.930600e+08  \n",
       "75%    1.870712e+09  \n",
       "max    1.110216e+10  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "stock_df = pd.read_csv(\"data/Nasdaq.csv\", index_col='Date')\n",
    "print(stock_df.head(10))\n",
    "# stock_df['Date'] = stock_df['Date'].apply(pd.to_datetime)\n",
    "stock_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at daily return histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "daily_return = np.empty(stock_df['Close'].shape)\n",
    "#  From Slides: Daily return (r): Difference in percentage between the price at time t+1 and at time t\n",
    "daily_return[0] = float('NaN') # The first\n",
    "daily_return[1:] = np.ediff1d(stock_df['Close']) / stock_df['Close'][:-1]\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Daily Return', value=daily_return)\n",
    "\n",
    "bins = int(len(daily_return) / 32)\n",
    "plt.hist(daily_return, bins=bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to compare binary and ternary solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "1971-02-05  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
      "1971-02-08  100.839996  100.839996  100.839996  100.839996  100.839996   \n",
      "1971-02-09  100.760002  100.760002  100.760002  100.760002  100.760002   \n",
      "1971-02-10  100.690002  100.690002  100.690002  100.690002  100.690002   \n",
      "1971-02-11  101.449997  101.449997  101.449997  101.449997  101.449997   \n",
      "1971-02-12  102.050003  102.050003  102.050003  102.050003  102.050003   \n",
      "1971-02-16  102.190002  102.190002  102.190002  102.190002  102.190002   \n",
      "1971-02-17  101.739998  101.739998  101.739998  101.739998  101.739998   \n",
      "1971-02-18  101.419998  101.419998  101.419998  101.419998  101.419998   \n",
      "1971-02-19  100.699997  100.699997  100.699997  100.699997  100.699997   \n",
      "1971-02-22   99.680000   99.680000   99.680000   99.680000   99.680000   \n",
      "1971-02-23   99.720001   99.720001   99.720001   99.720001   99.720001   \n",
      "1971-02-24  100.639999  100.639999  100.639999  100.639999  100.639999   \n",
      "1971-02-25  101.230003  101.230003  101.230003  101.230003  101.230003   \n",
      "1971-02-26  101.339996  101.339996  101.339996  101.339996  101.339996   \n",
      "1971-03-01  101.779999  101.779999  101.779999  101.779999  101.779999   \n",
      "1971-03-02  101.839996  101.839996  101.839996  101.839996  101.839996   \n",
      "1971-03-03  102.070000  102.070000  102.070000  102.070000  102.070000   \n",
      "1971-03-04  102.779999  102.779999  102.779999  102.779999  102.779999   \n",
      "1971-03-05  103.000000  103.000000  103.000000  103.000000  103.000000   \n",
      "\n",
      "            Volume  Daily Return  Binary Decision  Ternary Decision  \n",
      "Date                                                                 \n",
      "1971-02-05       0           NaN              0.0               0.0  \n",
      "1971-02-08       0      0.008400              1.0               2.0  \n",
      "1971-02-09       0     -0.000793              0.0               1.0  \n",
      "1971-02-10       0     -0.000695              0.0               1.0  \n",
      "1971-02-11       0      0.007548              1.0               2.0  \n",
      "1971-02-12       0      0.005914              1.0               2.0  \n",
      "1971-02-16       0      0.001372              1.0               2.0  \n",
      "1971-02-17       0     -0.004404              0.0               1.0  \n",
      "1971-02-18       0     -0.003145              0.0               1.0  \n",
      "1971-02-19       0     -0.007099              0.0               1.0  \n",
      "1971-02-22       0     -0.010129              0.0               1.0  \n",
      "1971-02-23       0      0.000401              1.0               1.0  \n",
      "1971-02-24       0      0.009226              1.0               2.0  \n",
      "1971-02-25       0      0.005863              1.0               2.0  \n",
      "1971-02-26       0      0.001087              1.0               2.0  \n",
      "1971-03-01       0      0.004342              1.0               2.0  \n",
      "1971-03-02       0      0.000589              1.0               1.0  \n",
      "1971-03-03       0      0.002258              1.0               2.0  \n",
      "1971-03-04       0      0.006956              1.0               2.0  \n",
      "1971-03-05       0      0.002141              1.0               2.0  \n"
     ]
    }
   ],
   "source": [
    "binary = (daily_return > 0).astype(float)\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Binary Decision', value=binary)\n",
    "\n",
    "limit = 0.001\n",
    "ternary = np.zeros(daily_return.shape)\n",
    "ternary[np.where(daily_return > limit)] = 2\n",
    "ternary[np.where(daily_return < limit)] = 1\n",
    "stock_df.insert(loc=len(stock_df.columns), column='Ternary Decision', value=ternary)\n",
    "\n",
    "print(stock_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1R model using volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DF:  12402\n",
      "Percentage of test_size to use last 100 days:  0.008063215610385421\n",
      "Binary Accuracy:  0.44\n"
     ]
    }
   ],
   "source": [
    "X_d_stock = stock_df[[\"Volume\"]]\n",
    "y_stock = stock_df[\"Binary Decision\"]\n",
    "print(\"Length of DF: \", stock_df.shape[0])\n",
    "test_size = 100/stock_df.shape[0]\n",
    "print(\"Percentage of test_size to use last 100 days: \", test_size)\n",
    "Xd_train_stock, Xd_test_stock, y_train_stock, y_test_stock = train_test_split(X_d_stock, y_stock, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock = OneRClassifier()\n",
    "oner_stock.fit(Xd_train_stock.to_numpy(), y_train_stock)\n",
    "y_pred_stock = oner_stock.predict(Xd_test_stock.to_numpy())\n",
    "accuracy_stock_bi = accuracy_score(y_test_stock, y_pred_stock)\n",
    "print(\"Binary Accuracy: \", accuracy_stock_bi)\n",
    "\n",
    "# As ternary does not play nicely with OneR, using binary only\n",
    "# X_d_stock_ter = stock_df[[\"Volume\"]]\n",
    "# y_stock_ter = stock_df[\"Ternary Decision\"]\n",
    "# Xd_train_stock_ter, Xd_test_stock_ter, y_train_stock_ter, y_test_stock_ter = train_test_split(X_d_stock_ter, y_stock_ter, test_size=0.3)\n",
    "# oner_stock_ter = OneRClassifier()\n",
    "# oner_stock_ter.fit(Xd_train_stock_ter.to_numpy(), y_train_stock_ter)\n",
    "# y_pred_stock_ter = oner_stock_ter.predict(Xd_test_stock_ter.to_numpy())\n",
    "# accuracy_stock_ter = accuracy_score(y_test_stock_ter, y_pred_stock_ter)\n",
    "# print(\"Ternary Accuracy: \", accuracy_stock_ter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1R model using volume and rolling averages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stock_df['Rolling Mean 5'] = stock_df['Close'].rolling(5).mean()\n",
    "stock_df['Rolling Mean 10'] = stock_df['Close'].rolling(10).mean()\n",
    "stock_df['Rolling Mean 20'] = stock_df['Close'].rolling(20).mean()\n",
    "stock_df['Rolling Mean 50'] = stock_df['Close'].rolling(50).mean()\n",
    "stock_df['Rolling Mean 200'] = stock_df['Close'].rolling(200).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Mean 5 Accuracy:  0.44\n",
      "Rolling Mean 10 Accuracy:  0.44\n",
      "Rolling Mean 20 Accuracy:  0.44\n",
      "Rolling Mean 50 Accuracy:  0.44\n",
      "Rolling Mean 200 Accuracy:  0.44\n"
     ]
    }
   ],
   "source": [
    "X_d_stock_5 = stock_df[[\"Volume\", \"Rolling Mean 5\"]]\n",
    "y_stock_5 = stock_df[\"Binary Decision\"]\n",
    "test_size = 100/stock_df.shape[0]\n",
    "Xd_train_stock_5, Xd_test_stock_5, y_train_stock_5, y_test_stock_5 = train_test_split(X_d_stock_5, y_stock_5, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_5 = OneRClassifier()\n",
    "oner_stock_5.fit(Xd_train_stock_5.to_numpy(), y_train_stock_5)\n",
    "y_pred_stock_5 = oner_stock_5.predict(Xd_test_stock_5.to_numpy())\n",
    "accuracy_stock_bi_5 = accuracy_score(y_test_stock_5, y_pred_stock_5)\n",
    "print(\"Rolling Mean 5 Accuracy: \", accuracy_stock_bi_5)\n",
    "\n",
    "X_d_stock_10 = stock_df[[\"Volume\", \"Rolling Mean 10\"]]\n",
    "y_stock_10 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_10, Xd_test_stock_10, y_train_stock_10, y_test_stock_10 = train_test_split(X_d_stock_10, y_stock_10, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_10 = OneRClassifier()\n",
    "oner_stock_10.fit(Xd_train_stock_10.to_numpy(), y_train_stock_10)\n",
    "y_pred_stock_10 = oner_stock.predict(Xd_test_stock_10.to_numpy())\n",
    "accuracy_stock_bi_10 = accuracy_score(y_test_stock_10, y_pred_stock_10)\n",
    "print(\"Rolling Mean 10 Accuracy: \", accuracy_stock_bi_10)\n",
    "\n",
    "X_d_stock_20 = stock_df[[\"Volume\", \"Rolling Mean 20\"]]\n",
    "y_stock_20 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_20, Xd_test_stock_20, y_train_stock_20, y_test_stock_20 = train_test_split(X_d_stock_20, y_stock_20, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_20 = OneRClassifier()\n",
    "oner_stock_20.fit(Xd_train_stock_20.to_numpy(), y_train_stock_20)\n",
    "y_pred_stock_20 = oner_stock_20.predict(Xd_test_stock_20.to_numpy())\n",
    "accuracy_stock_bi_20 = accuracy_score(y_test_stock_20, y_pred_stock_20)\n",
    "print(\"Rolling Mean 20 Accuracy: \", accuracy_stock_bi_20)\n",
    "\n",
    "X_d_stock_50 = stock_df[[\"Volume\", \"Rolling Mean 50\"]]\n",
    "y_stock_50 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_50, Xd_test_stock_50, y_train_stock_50, y_test_stock_50 = train_test_split(X_d_stock_50, y_stock_50, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_50 = OneRClassifier()\n",
    "oner_stock_50.fit(Xd_train_stock_50.to_numpy(), y_train_stock_50)\n",
    "y_pred_stock_50 = oner_stock_50.predict(Xd_test_stock_50.to_numpy())\n",
    "accuracy_stock_bi_50 = accuracy_score(y_test_stock_50, y_pred_stock_50)\n",
    "print(\"Rolling Mean 50 Accuracy: \", accuracy_stock_bi_50)\n",
    "\n",
    "X_d_stock_200 = stock_df[[\"Volume\", \"Rolling Mean 200\"]]\n",
    "y_stock_200 = stock_df[\"Binary Decision\"]\n",
    "Xd_train_stock_200, Xd_test_stock_200, y_train_stock_200, y_test_stock_200 = train_test_split(X_d_stock_200, y_stock_200, test_size=test_size, random_state=2, shuffle=False)\n",
    "oner_stock_200 = OneRClassifier()\n",
    "oner_stock_200.fit(Xd_train_stock_200.to_numpy(), y_train_stock_200)\n",
    "y_pred_stock_200 = oner_stock_200.predict(Xd_test_stock_200.to_numpy())\n",
    "accuracy_stock_bi_200 = accuracy_score(y_test_stock_200, y_pred_stock_200)\n",
    "print(\"Rolling Mean 200 Accuracy: \", accuracy_stock_bi_200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weirdly enough, no matter what the rolling average, we seem to receive the same results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}